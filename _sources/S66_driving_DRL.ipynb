{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "tags": [
     "no-tex"
    ]
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gtbook/robotics/blob/main/S66_driving_DRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JoW4C_OkOMhe",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U gtbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "10-snNDwOSuC",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# no imports yet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nAvx4-UCNzt2"
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "> Deep Q-learning and policy gradient.\n",
    "\n",
    "<img src=\"Figures6/S66-Autonomous_Vehicle_with_LIDAR_and_cameras-03.jpg\" alt=\"Splash image with steampunk autonomous car\" width=\"40%\" align=center style=\"vertical-align:middle;margin:10px 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL and Autonomous Driving\n",
    "\n",
    "TODO: add 3-lane highway figure\n",
    "\n",
    "An simple example in the autonomous driving domain is *lane switching*. Suppose we are driving along at 3-lane highway, and we can see some ways ahead, and some ways behind us. We are driving at a speed that is comfortable to us, but other cars have different ideas about the optimal speed to drive at. Hence, sometimes we would like to change lanes, and we could learn a policy to do this for us. This is called **lateral control**. A more sophisticated example would also allow us to adapt our speed to the traffic pattern, but by relying on a smart cruise control system we could safely ignore this **longitudinal control** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: figure with digitized occupancy\n",
    "\n",
    "To turn this into a reinforcement learning problem, we first need to define a state space ${\\cal X}$ and an action space ${\\cal A}$. There are a variety of ways to engineer this aspect of the problem. For example, we could somehow encode the longitudinal distance to the lane index for each of the K closest cars, where K is a parameter, say 5 or 10. One problem is that the number of cars that are *actually* present is variable, which is difficult to deal with. Another approach is to make this into an image processing problem, by creating a finite element representation of the road before and behind us, and marking a cell as occupied or not. The latter is fairly compatible with automotive sensors such as LIDAR, and this is the one that we will adopt.\n",
    "\n",
    "TODO: figure with actions: LEFT, RIGHT, STAY\n",
    "\n",
    "In terms of actions, the easiest approach is to have a number of discrete choices to go `left`, `right`, or `stay` in the current lane. We could be more sophisticated about it and have both \"aggressive\" and \"slow\" versions of these in addition to a default version, akin to the motion primitives previously discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "> DQN is an early deep learning RL method akin to Q-learning.\n",
    "\n",
    "Recall from Section 3.6 that we can define a policy in terms of **Q-values**, sometimes also called state-action values, and that we can define the optimal policy as \n",
    "\n",
    "$$\n",
    "\\pi^*(x) = \\arg \\max_a Q^*(x,a)\n",
    "$$\n",
    "\n",
    "In Q-learning, we start with some random Q-values and then gradually estimate the optimal Q-values by alpha-blending between old and new estimates:\n",
    "\n",
    "$$\n",
    "\\hat{Q}(x,a) \\leftarrow (1-\\alpha) \\hat{Q}(x,a) + \\alpha~\\text{target}(x,a,x')\n",
    "$$\n",
    "\n",
    "where $\\text{target}(x,a,x') \\doteq R(x,a,x') + \\gamma \\max_{a'} \\hat{Q}(x',a')$ is the \"target\" value that we think is an improvment on the previous value $\\hat{Q}(x,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **deep Q-network** or DQN method we use a *supervised learning* approach to Q-learning, by training a neural network, parameterized by $\\theta$, to approximate the optimal Q-values:\n",
    "\n",
    "$$\n",
    "Q^*(x,a) \\approx Q(x,a; \\theta)\n",
    "$$\n",
    "\n",
    "DQN as a method uses two additional ideas that are crucial in making the training converge to something sensible in difficult problems. The first is splitting the training into *execution* and *experience replay* phases:\n",
    "\n",
    "- during the **execution phase**, it executes the policy (possibly with some degree of randomness) and stores the experiences $(x,a,r,x')$, with $r$ the reward, in a dataset $D$;\n",
    "- during **experience replay**, it *randomly samples* from these experiences to create mini-batches of data, which are in turn used to perform stochastic gradient descent on the parameters $\\theta$.\n",
    "\n",
    "The second is to calculate the target values $\\text{target}(x,a,x') \\doteq R(x,a,x') + \\gamma \\max_{a'} \\hat{Q}(x',a'; \\theta^{old})$ with the parameters $\\theta^{old}$ from the previous epoch, to provide a more stable approximation target.\n",
    "\n",
    "With this basic scheme, a team from DeepMind was able to achieve human or super-human performance on about 50 Atari 2600 games in 2015 (Minh et al., Nature 2015, Vol. 518)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization\n",
    "\n",
    "> Policy optimization takes a black box optimization approach to a deep policy.\n",
    "\n",
    "We can use a deep neural network, parameterized by $\\theta$, to encode a **stochastic policy**:\n",
    "\n",
    "$$\n",
    "\\pi(a|x, \\theta)\n",
    "$$\n",
    "\n",
    "where $a \\in {\\cal A}$ is an action, $x \\in {\\cal X}$ is a state, and the policy outputs a *probability* for each action $a$ based on the state $x$.\n",
    "\n",
    "Similar to policy iteration, policy optimization tries to find the policy $\\pi$ that maximizes the expected utility. One way to estimate this quantity is to do a rollout for a finite horizon $H$, as we discussed in Chapter 3. An often used loss function when stochastic policy parameters $\\theta$ being optimized for is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\sum_{t=0}^H R_t - \\log \\pi(a_t|x_t, \\theta)\n",
    "$$\n",
    "\n",
    "where $R^t$ is the expected return for the $t^{th}$ transition in the rollout, and is computed as\n",
    "\n",
    "$$\n",
    "R^t \\doteq \\sum_{k=t}^H \\gamma^{k-t}R(x_t,a_t,x_{t+1})\n",
    "$$\n",
    "\n",
    "The reason to use $\\log \\pi(a_t|x_t, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple optimization method is **hill climbing**:\n",
    "\n",
    "- initialize $\\theta$ and compute $\\mathcal{L}(\\theta)$.\n",
    "- until convergence:\n",
    "    - perturb $\\theta$ to $\\theta'$\n",
    "    - if $\\mathcal{L}(\\theta') < \\mathcal{L}(\\theta)$:\n",
    "        - set $\\theta \\leftarrow \\theta`$\n",
    "\n",
    "Above the *perturbation* step could be as simple as adding some Gaussian noise to the weights of the network.\n",
    "\n",
    "More sophisticated \"black-box\" approaches such as genetic algorithms or evolution strategies can be applied to this problem instead, but they all share the property that they are \"gradient-free\", which seems to be a sub-optimal strategy. Hence, we next look at a gradient descent approach to policy optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "> Policy gradient methods are akin to policy iteration, with a deep twist.\n",
    "\n",
    "In a nutshell, policy gradient calculates the gradient of the loss with respect to the policy parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}(\\theta) \\leftarrow \\sum_{t=0}^H - R_t \\nabla_\\theta \\log \\pi(a_t|x_t, \\theta)\n",
    "$$\n",
    "\n",
    "and then uses gradient descent to update the policy parameters:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a learning rate.\n",
    "\n",
    "There are a whole host of reasons why the above, vanilla **policy gradient algorithm**, does not work well. Hence, there is a large variety of methods that try to improve the performance of this algorithm, beginning with the 1992 REINFORCE method (Williams, 1992), up to more modern methods such as PPO, which is now one of the most often-used methods.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "S66_driving_DRL.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_metadata": {
   "affiliation": "Georgia Institute of Technology",
   "author": "Frank Dellaert and Seth Hutchinson",
   "title": "Introduction to Robotics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
